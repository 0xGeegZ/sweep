{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates on the Chunking Algorithm\n",
    "This notebook is for the blog on improvements to our chunking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "\n",
    "language = get_language('python')\n",
    "parser = get_parser('python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Span:\n",
    "    start: int = 0\n",
    "    end: int = field(default=lambda self: self.start)\n",
    "\n",
    "    def extract(self, s: str) -> str:\n",
    "        return \"\\n\".join(s.splitlines()[self.start:self.end])\n",
    "\n",
    "    def __add__(self, other) -> Span:\n",
    "        if isinstance(other, int):\n",
    "            return Span(self.start + other, self.end + other)\n",
    "        elif isinstance(other, Span):\n",
    "            return Span(self.start, other.end)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code we're gonna use in this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression_statement:432-696\n",
      "  assignment:432-696\n",
      "    string:446-696\n",
      "function_definition:698-1502\n",
      "  block:777-1502\n",
      "    expression_statement:777-953\n",
      "      assignment:777-953\n",
      "        dictionary:787-953\n",
      "    expression_statement:958-1103\n",
      "      assignment:958-1103\n",
      "        call:969-1103\n",
      "    if_statement:1127-1482\n",
      "      block:1167-1400\n",
      "        for_statement:1232-1400\n",
      "function_definition:1505-2107\n",
      "  block:1540-2107\n",
      "    expression_statement:1643-1994\n",
      "      assignment:1643-1994\n",
      "        list:1654-1994\n",
      "function_definition:2110-4426\n",
      "  block:2162-4426\n",
      "    if_statement:2355-2512\n",
      "      block:2393-2512\n",
      "    if_statement:3212-3397\n",
      "      block:3258-3397\n",
      "        expression_statement:3258-3397\n",
      "          augmented_assignment:3258-3397\n",
      "    if_statement:3447-3859\n",
      "      block:3563-3859\n",
      "        expression_statement:3563-3746\n",
      "          assignment:3563-3746\n",
      "            call:3573-3746\n",
      "              argument_list:3601-3746\n",
      "                binary_operator:3602-3745\n",
      "    expression_statement:3984-4397\n",
      "      call:3984-4397\n",
      "        argument_list:3994-4397\n"
     ]
    }
   ],
   "source": [
    "python_code = r'''\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "from loguru import logger\n",
    "\n",
    "from sweepai.core.gha_extraction import GHAExtractor\n",
    "from sweepai.events import CheckRunCompleted\n",
    "from sweepai.handlers.on_comment import on_comment\n",
    "from sweepai.utils.config.client import SweepConfig, get_gha_enabled\n",
    "from sweepai.utils.github_utils import get_github_client, get_token\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "log_message = \"\"\"GitHub actions yielded the following error. \n",
    "\n",
    "{error_logs}\n",
    "\n",
    "This is likely a linting or type-checking issue with the source code but if you are updating the GitHub Actions or versioning, this could be an issue with the GitHub Action yaml files.\"\"\"\n",
    "\n",
    "def download_logs(repo_full_name: str, run_id: int, installation_id: int):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {get_token(installation_id)}\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
    "    }\n",
    "    response = requests.get(f\"https://api.github.com/repos/{repo_full_name}/actions/runs/{run_id}/logs\",\n",
    "                            headers=headers)\n",
    "\n",
    "    logs_str = \"\"\n",
    "    if response.status_code == 200:\n",
    "        zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        for file in zip_file.namelist():\n",
    "            if \"/\" not in file:\n",
    "                with zip_file.open(file) as f:\n",
    "                    logs_str += f.read().decode(\"utf-8\")\n",
    "    else:\n",
    "        logger.warning(f\"Failed to download logs for run id: {run_id}\")\n",
    "    return logs_str\n",
    "\n",
    "\n",
    "def clean_logs(logs_str: str):\n",
    "    log_list = logs_str.split(\"\\n\")\n",
    "    truncated_logs = [log[log.find(\" \") + 1:] for log in log_list]\n",
    "    patterns = [\n",
    "        # for docker\n",
    "        \"Already exists\",\n",
    "        \"Pulling fs layer\",\n",
    "        \"Waiting\",\n",
    "        \"Download complete\",\n",
    "        \"Verifying Checksum\",\n",
    "        \"Pull complete\",\n",
    "        # For github\n",
    "        \"remote: Counting objects\",\n",
    "        \"remote: Compressing objects:\",\n",
    "        \"Receiving objects:\",\n",
    "        \"Resolving deltas:\"\n",
    "    ]\n",
    "    return \"\\n\".join([log.strip() for log in truncated_logs if not any(pattern in log for pattern in patterns)])\n",
    "\n",
    "\n",
    "def on_check_suite(request: CheckRunCompleted):\n",
    "    logger.info(f\"Received check run completed event for {request.repository.full_name}\")\n",
    "    g = get_github_client(request.installation.id)\n",
    "    repo = g.get_repo(request.repository.full_name)\n",
    "    if not get_gha_enabled(repo):\n",
    "        logger.info(f\"Skipping github action for {request.repository.full_name} because it is not enabled\")\n",
    "        return None\n",
    "    pr = repo.get_pull(request.check_run.pull_requests[0].number)\n",
    "    num_pr_commits = len(list(pr.get_commits()))\n",
    "    if num_pr_commits > 20:\n",
    "        logger.info(f\"Skipping github action for PR with {num_pr_commits} commits\")\n",
    "        return None\n",
    "    logger.info(f\"Running github action for PR with {num_pr_commits} commits\")\n",
    "    logs = download_logs(\n",
    "        request.repository.full_name,\n",
    "        request.check_run.run_id,\n",
    "        request.installation.id\n",
    "    )\n",
    "    if not logs:\n",
    "        return None\n",
    "    logs = clean_logs(logs)\n",
    "    extractor = GHAExtractor()\n",
    "    logger.info(f\"Extracting logs from {request.repository.full_name}, logs: {logs}\")\n",
    "    problematic_logs = extractor.gha_extract(logs)\n",
    "    if problematic_logs.count(\"\\n\") > 15:\n",
    "        problematic_logs += \"\\n\\nThere are a lot of errors. This is likely a larger issue with the PR and not a small linting/type-checking issue.\"\n",
    "    comments = list(pr.get_issue_comments())\n",
    "    if len(comments) >= 2 and problematic_logs == comments[-1].body and comments[-2].body == comments[-1].body:\n",
    "        comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs) + \"\\n\\nI'm getting the same errors 3 times in a row, so I will stop working on fixing this PR.\")\n",
    "        logger.warning(\"Skipping logs because it is duplicated\")\n",
    "        raise Exception(\"Duplicate error logs\")\n",
    "    print(problematic_logs)\n",
    "    comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs))\n",
    "    on_comment(\n",
    "        repo_full_name=request.repository.full_name,\n",
    "        repo_description=request.repository.description,\n",
    "        comment=problematic_logs,\n",
    "        pr_path=None,\n",
    "        pr_line_position=None,\n",
    "        username=request.sender.login,\n",
    "        installation_id=request.installation.id,\n",
    "        pr_number=request.check_run.pull_requests[0].number,\n",
    "        comment_id=comment.id,\n",
    "        repo=repo,\n",
    "    )\n",
    "    return {\"success\": True}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the syntax tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parser.parse(python_code.encode(\"utf-8\"))\n",
    "\n",
    "def pretty_node(node):\n",
    "    return f\"{node.type}:{node.start_byte}-{node.end_byte}\"\n",
    "\n",
    "def print_tree(node, indent=\"\"):\n",
    "    if len(re.sub(\"\\s\", \"\", node.text.decode(\"utf-8\"))) < 100:\n",
    "        return\n",
    "    print(indent + pretty_node(node))\n",
    "    for child in node.children:\n",
    "        print_tree(child, indent=indent + \"  \")\n",
    "\n",
    "for child in tree.root_node.children:\n",
    "    print_tree(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it doesn't actually line up:\n",
    "\n",
    "```\n",
    "expression_statement:432-696 : log_message = \"...\n",
    "  assignment:432-696 : log_message = \"...\n",
    "    string:446-696 : \"\"\"GitHub actio...\n",
    "function_definition:698-1502 : def download_lo...\n",
    "  block:777-1502 : headers = {    ...\n",
    "    expression_statement:777-953 : headers = {    ...\n",
    "      assignment:777-953 : headers = {    ...\n",
    "        dictionary:787-953 : {         \"Acce...\n",
    "    expression_statement:958-1103 : response = requ...\n",
    "      assignment:958-1103 : response = requ...\n",
    "```\n",
    "\n",
    "Notice that the “expression_statement” ends on byte 696 and “function_definition” starts on byte 698, skipping a byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chunks(chunks: list[Span]):\n",
    "    for prev, curr in zip(chunks[:-1], chunks[1:]):\n",
    "        prev.end = curr.start\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coalescing\n",
    "Recall this is the old algo (with some fixes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tree_sitter.Node() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 38\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39m# # Handle last child\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m# if next_child.end_byte - next_child.start_byte > MAX_CHARS:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m#     new_chunks.append(current_chunk)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39m#     new_chunks.append(current_chunk + next_child.text.decode(\"utf-8\"))\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39m# new_chunks.append(current_chunk)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m new_chunks\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunk_node(tree\u001b[39m.\u001b[39;49mroot_node, python_code):\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(chunk \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m====================\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[94], line 10\u001b[0m, in \u001b[0;36mchunk_node\u001b[0;34m(node, text, MAX_CHARS)\u001b[0m\n\u001b[1;32m      8\u001b[0m new_chunks \u001b[39m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m current_chunk \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m final_node \u001b[39m=\u001b[39m Node(node\u001b[39m.\u001b[39;49mstart_byte, node\u001b[39m.\u001b[39;49mend_byte)\n\u001b[1;32m     11\u001b[0m \u001b[39m# final_node.start_byte = node.end_byte\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# final_node.end_byte = node.end_byte\u001b[39;00m\n\u001b[1;32m     13\u001b[0m node_children \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mchildren \u001b[39m+\u001b[39m [final_node]\n",
      "\u001b[0;31mTypeError\u001b[0m: tree_sitter.Node() takes no arguments"
     ]
    }
   ],
   "source": [
    "from tree_sitter import Node\n",
    "\n",
    "@dataclass\n",
    "class MockNode:\n",
    "    start_byte: int = 0\n",
    "    end_byte: int = field(default_factory=lambda self: self.start_byte + 1)\n",
    "    children: list[MockNode] = field(default_factory=list)\n",
    "\n",
    "def chunk_node(\n",
    "    node: Node, \n",
    "    text: str, \n",
    "    MAX_CHARS: int = 600\n",
    ") -> list[str]:\n",
    "    new_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    node_children = node.children + [MockNode(node.end_byte, node.end_byte, [])]\n",
    "\n",
    "    for child, next_child in zip(node.children[:-1], node.children[1:]):\n",
    "        if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            new_chunks.extend(chunk_node(child, text, MAX_CHARS))\n",
    "        elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = text[child.start_byte: next_child.start_byte]\n",
    "        else:\n",
    "            current_chunk += text[child.start_byte: next_child.start_byte]\n",
    "    \n",
    "    # # Handle last child\n",
    "    # if next_child.end_byte - next_child.start_byte > MAX_CHARS:\n",
    "    #     new_chunks.append(current_chunk)\n",
    "    #     new_chunks.extend(chunk_node(next_child, text, MAX_CHARS))\n",
    "    # elif next_child.end_byte - next_child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "    #     new_chunks.append(current_chunk)\n",
    "    #     new_chunks.append(next_child.text.decode(\"utf-8\"))\n",
    "    # else:\n",
    "    #     new_chunks.append(current_chunk + next_child.text.decode(\"utf-8\"))\n",
    "    # new_chunks.append(current_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "for chunk in chunk_node(tree.root_node, python_code):\n",
    "    print(chunk + \"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, using Span's we can clean up the code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Node\n",
    "\n",
    "def chunk_node(\n",
    "    node: Node, \n",
    "    text: str, \n",
    "    MAX_CHARS: int = 600\n",
    ") -> list[Span]:\n",
    "    new_chunks: list[Span] = []\n",
    "    current_chunk: Span = Span()\n",
    "    for child in node.children:\n",
    "        if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = Span(child.end_byte + 1)\n",
    "            new_chunks.extend(chunk_node(child, text, MAX_CHARS))\n",
    "        elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = Span(child.start_byte, child.end_byte) \n",
    "        else:\n",
    "            current_chunk += Span(child.start_byte, child.end_byte)\n",
    "    \n",
    "    # Handle last child\n",
    "    if next_child.end_byte - next_child.start_byte > MAX_CHARS:\n",
    "        new_chunks.append(current_chunk)\n",
    "        new_chunks.extend(chunk_node(next_child, text, MAX_CHARS))\n",
    "    elif next_child.end_byte - next_child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "        new_chunks.append(current_chunk)\n",
    "        new_chunks.append(next_child.text.decode(\"utf-8\"))\n",
    "    else:\n",
    "        new_chunks.append(current_chunk + next_child.text.decode(\"utf-8\"))\n",
    "    new_chunks.append(current_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "for chunk in chunk_node(tree.root_node, python_code):\n",
    "    print(chunk + \"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('sweepai-u_CIt3kb-py3.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f22e56b0c638c2a35876232f2e2d6cfc31a0d98b7f3049980f1a4383610dba30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
